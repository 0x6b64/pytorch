opcode         name          target                   args                       kwargs
-------------  ------------  -----------------------  -------------------------  --------
placeholder    arg0          arg0                     ()                         {}
call_function  sym_size      aten.sym_size            (arg0, 0)                  {}
call_function  sym_size_1    aten.sym_size            (arg0, 1)                  {}
call_function  mul           <built-in function mul>  (sym_size, sym_size_1)     {}
call_function  sym_size_2    aten.sym_size            (arg0, 2)                  {}
call_function  view_default  aten.view.default        (arg0, [mul, sym_size_2])  {}
output         output        output                   ([view_default],)          {}
class GraphModule(torch.nn.Module):
    def forward(self, orig_arg_0):
        arg0: f32[s0, s1, s2], = fx_pytree.tree_flatten_spec([orig_arg_0], self._in_spec)
        # File: /scratch/bahuang/work/repos/pytorch/example_symint.py:31, code: a =  x.view(new_shape)      # SymInt[] as arg
        sym_size: Sym(s0) = torch.ops.aten.sym_size(arg0, 0)
        sym_size_1: Sym(s1) = torch.ops.aten.sym_size(arg0, 1)
        
        # No stacktrace found for following nodes
        mul: Sym(s0*s1) = sym_size * sym_size_1;  sym_size = sym_size_1 = None
        
        # File: /scratch/bahuang/work/repos/pytorch/example_symint.py:31, code: a =  x.view(new_shape)      # SymInt[] as arg
        sym_size_2: Sym(s2) = torch.ops.aten.sym_size(arg0, 2)
        view_default: f32[s0*s1, s2] = torch.ops.aten.view.default(arg0, [mul, sym_size_2]);  arg0 = mul = sym_size_2 = None
        return pytree.tree_unflatten([view_default], self._out_spec)
        
GraphModule(
    name='GraphModule',
    graph=Graph(
        inputs=[TensorArgument(name='arg0')],
        outputs=[TensorArgument(name='view_default')],
        nodes=[
            Node(
                op='call_function',
                target='aten.sym_size',
                args=[
                    Argument(as_tensor=TensorArgument(name='arg0')),
                    Argument(as_int=0)
                ],
                kwargs={},
                outputs=[
                    ReturnArgument(as_symint=SymIntArgument(name='sym_size'))
                ],
                metadata='Skipped'
            ),
            Node(
                op='call_function',
                target='aten.sym_size',
                args=[
                    Argument(as_tensor=TensorArgument(name='arg0')),
                    Argument(as_int=1)
                ],
                kwargs={},
                outputs=[
                    ReturnArgument(as_symint=SymIntArgument(name='sym_size_1'))
                ],
                metadata='Skipped'
            ),
            Node(
                op='call_function',
                target='<built-in function mul>',
                args=[
                    Argument(as_symint=SymIntArgument(name='sym_size')),
                    Argument(as_symint=SymIntArgument(name='sym_size_1'))
                ],
                kwargs={},
                outputs=[ReturnArgument(as_symint=SymIntArgument(name='mul'))],
                metadata='Skipped'
            ),
            Node(
                op='call_function',
                target='aten.sym_size',
                args=[
                    Argument(as_tensor=TensorArgument(name='arg0')),
                    Argument(as_int=2)
                ],
                kwargs={},
                outputs=[
                    ReturnArgument(as_symint=SymIntArgument(name='sym_size_2'))
                ],
                metadata='Skipped'
            ),
            Node(
                op='call_function',
                target='aten.view.default',
                args=[
                    Argument(as_tensor=TensorArgument(name='arg0')),
                    Argument(
                        as_symints=[
                            SymIntArgument(name='mul'),
                            SymIntArgument(name='sym_size_2')
                        ]
                    )
                ],
                kwargs={},
                outputs=[
                    ReturnArgument(
                        as_tensor=TensorArgument(name='view_default')
                    )
                ],
                metadata='Skipped'
            )
        ],
        ivalues=[
            IValue(
                name='arg0',
                meta=TensorMeta(
                    dtype=torch.float32,
                    sizes=[
                        SymInt(as_sym=s0),
                        SymInt(as_sym=s1),
                        SymInt(as_sym=s2)
                    ],
                    requires_grad=False,
                    device=device(type='cpu'),
                    strides=[
                        SymInt(as_sym=s1*s2),
                        SymInt(as_sym=s2),
                        SymInt(as_sym=1)
                    ],
                    storage_offset=SymInt(as_sym=0),
                    layout=torch.strided
                )
            ),
            IValue(
                name='view_default',
                meta=TensorMeta(
                    dtype=torch.float32,
                    sizes=[SymInt(as_sym=s0*s1), SymInt(as_sym=s2)],
                    requires_grad=False,
                    device=device(type='cpu'),
                    strides=[SymInt(as_sym=s2), SymInt(as_sym=1)],
                    storage_offset=SymInt(as_sym=0),
                    layout=torch.strided
                )
            )
        ],
        symint_values={
            'sym_size': SymInt(as_int=4),
            'sym_size_1': SymInt(as_int=3),
            'mul': SymInt(as_int=12),
            'sym_size_2': SymInt(as_int=2)
        }
    ),
    metadata={},
    parameters='Skipped',
    buffers='Skipped'
)
